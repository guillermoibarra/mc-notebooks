{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb6b57e-a7cb-4dee-8fed-8b6003dd7714",
   "metadata": {},
   "source": [
    "# 1. Introducción general  \n",
    "\n",
    "El método de Monte Carlo es una técnica computacional basada en muestreos aleatorios repetidos para aproximar soluciones numéricas de problemas que pueden ser difíciles o imposibles de resolver analíticamente. Se utiliza, por ejemplo, para estimar integrales, resolver problemas de optimización o estudiar sistemas con incertidumbre. De forma general, el método consiste en:\n",
    "\n",
    "1. Definir una región o un conjunto de parámetros.  \n",
    "2. Tomar muestras al azar dentro de ese espacio.  \n",
    "3. Calcular una función determinista en cada valor muestreado.  \n",
    "4. Promediar los resultados para aproximar la solución buscada.\n",
    "\n",
    "## Formulación matemática\n",
    "\n",
    "Para ilustrar el método, considérese el problema de integración unidimensional:\n",
    "\n",
    "$$\n",
    "I = \\int_a^b f(x)\\,dx.\n",
    "$$\n",
    "\n",
    "Si se eligen $N$ puntos $x_i$ aleatoriamente y de manera uniforme en $[a,b]$, la estimación de Monte Carlo para $I$ es:\n",
    "\n",
    "$$\n",
    "\\hat{I} \\approx (b - a) \\,\\frac{1}{N} \\sum_{i=1}^N f(x_i).\n",
    "$$\n",
    "\n",
    "A medida que $N$ crece, $\\hat{I}$ se aproxima al valor real de la integral gracias a la ley de los grandes números. Este mismo principio se extiende a integrales en múltiples dimensiones muestreando en un dominio de dimensión superior y multiplicando por el volumen correspondiente.\n",
    "\n",
    "## Ejemplos en Python\n",
    "\n",
    "### Ejemplo en 1D  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9603c5-cd84-437c-aa83-4712f901733a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado Monte Carlo 1D: 0.337046628956089\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def monte_carlo_1d(func, a, b, num_samples):\n",
    "    # Generar muestras en [a, b]\n",
    "    samples = np.random.uniform(a, b, num_samples)\n",
    "    # Evaluar la función en cada muestra\n",
    "    evaluations = func(samples)\n",
    "    # Calcular el promedio y multiplicar por la longitud [b - a]\n",
    "    integral = (b - a) * np.mean(evaluations)\n",
    "    return integral\n",
    "\n",
    "# Ejemplo: f(x) = x^2 en [0,1]\n",
    "def f_1d(x):\n",
    "    return x**2\n",
    "\n",
    "result_1d = monte_carlo_1d(f_1d, 0, 1, 10000)\n",
    "print(\"Resultado Monte Carlo 1D:\", result_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb30e8-6931-480d-b6ba-a486be1c0691",
   "metadata": {},
   "source": [
    "### Ejemplo en 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f7de01c-bdb2-4dac-830d-ea9e4c910bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado Monte Carlo 2D: 0.6682082710916482\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def monte_carlo_2d(func, x_bounds, y_bounds, num_samples):\n",
    "    # Generar muestras en [x_bounds[0], x_bounds[1]] y [y_bounds[0], y_bounds[1]]\n",
    "    x_samples = np.random.uniform(x_bounds[0], x_bounds[1], num_samples)\n",
    "    y_samples = np.random.uniform(y_bounds[0], y_bounds[1], num_samples)\n",
    "    # Evaluar la función\n",
    "    evaluations = func(x_samples, y_samples)\n",
    "    # Calcular el área y aproximar la integral\n",
    "    area = (x_bounds[1] - x_bounds[0]) * (y_bounds[1] - y_bounds[0])\n",
    "    integral = area * np.mean(evaluations)\n",
    "    return integral\n",
    "\n",
    "# Ejemplo: f(x, y) = x^2 + y^2 en [0,1] x [0,1]\n",
    "def f_2d(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "result_2d = monte_carlo_2d(f_2d, (0, 1), (0, 1), 10000)\n",
    "print(\"Resultado Monte Carlo 2D:\", result_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49975bbf-a424-4aba-aa7e-8c13780c4007",
   "metadata": {},
   "source": [
    "### Ejemplo en N Dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11b295e-b146-4f2d-a790-1dc529fcb49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado Monte Carlo 3D: 0.9871917562783714\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def monte_carlo_nd(func, bounds, num_samples):\n",
    "    dimensions = len(bounds)\n",
    "    # Generar muestras aleatorias en cada dimensión\n",
    "    samples = np.zeros((num_samples, dimensions))\n",
    "    for i in range(dimensions):\n",
    "        low, high = bounds[i]\n",
    "        samples[:, i] = np.random.uniform(low, high, num_samples)\n",
    "    # Evaluar la función en cada muestra\n",
    "    evaluations = func(samples)\n",
    "    # Calcular el volumen del hiper-rectángulo\n",
    "    volume = 1\n",
    "    for low, high in bounds:\n",
    "        volume *= (high - low)\n",
    "    # Aproximar la integral\n",
    "    integral = volume * np.mean(evaluations)\n",
    "    return integral\n",
    "\n",
    "# Ejemplo: f(x, y, z) = x^2 + y^2 + z^2 en [0,1]^3\n",
    "def f_nd(points):\n",
    "    return np.sum(points**2, axis=1)\n",
    "\n",
    "bounds_3d = [(0, 1), (0, 1), (0, 1)]\n",
    "result_nd = monte_carlo_nd(f_nd, bounds_3d, 10000)\n",
    "print(\"Resultado Monte Carlo 3D:\", result_nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5bfe77-614a-4f00-bb27-0e535fadeb03",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "**Ejercicio 1**  \n",
    "\n",
    "- Implementa la integración de Monte Carlo en 1D para una función sencilla (por ejemplo, f(x) = x² en el intervalo [0,1]).  \n",
    "- Varía el número de muestras (e.g., 100, 1000, 10000) y compara cómo cambia la estimación de la integral.  \n",
    "- Mide el tiempo de ejecución para cada variación y observa el compromiso entre exactitud y tiempo de cómputo.\n",
    "\n",
    "**Ejercicio 2**  \n",
    "\n",
    "- Emplea un método de Monte Carlo en 2D para estimar el área de un cuarto de círculo de radio 1 dentro del cuadrado [0,1]×[0,1].  \n",
    "- Muestra puntos uniformemente en esa región y cuenta cuántos caen dentro de x² + y² ≤ 1.  \n",
    "- Observa cómo la estimación se acerca a π/4 conforme aumenta el número de muestras.  \n",
    "- Representa los puntos de un experimento para visualizar la dispersión.\n",
    "\n",
    "**Ejercicio 3**  \n",
    "\n",
    "- Modifica el ejemplo en 2D para aproximar la integral de f(x, y) = sin(x)*cos(y) en algún dominio, por ejemplo [0,1]×[0,1].  \n",
    "- Compara el resultado calculado por Monte Carlo con algún método de integración numérica más preciso (por ejemplo, usando librerías de Python).  \n",
    "- Observa la convergencia de la estimación al aumentar el número de muestras.\n",
    "\n",
    "**Ejercicio 4**  \n",
    "\n",
    "- Extiende la integración de Monte Carlo a N dimensiones para la función f(x₁, x₂, …, xₙ) = ∑(xᵢ²) en la hipercaja [0,1]ⁿ.  \n",
    "- Comienza con n = 3 y aumenta hasta n = 5 o más.  \n",
    "- Observa cómo varía la precisión con el incremento de la dimensión manteniendo fijo el número de muestras.  \n",
    "- Explica brevemente el fenómeno conocido como “la maldición de la dimensionalidad” y cómo afecta a los métodos de Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a141c0f-48a4-428d-8026-9a598c4ed039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a800748-a467-4870-a182-d11530977ee9",
   "metadata": {},
   "source": [
    "# 2. Muestreo de Importancia\n",
    "\n",
    "Los métodos de Monte Carlo permiten aproximar cantidades de interés mediante muestreo repetido de una distribución de probabilidad. El muestreo de importancia (importance sampling) es una técnica que puede mejorar la eficiencia concentrando las muestras en las regiones más “relevantes” del espacio que más contribuyen a la cantidad deseada. Esto resulta especialmente ventajoso cuando la distribución de interés es difícil de muestrear directamente o cuando ciertos eventos raros influyen fuertemente en el resultado.\n",
    "\n",
    "## Formulación Matemática\n",
    "\n",
    "1. **Estimador Monte Carlo Estándar**  \n",
    "   Si se desea calcular $\\mathbb{E}_{p}[f(X)]$, donde $X \\sim p$, un estimador Monte Carlo convencional es  \n",
    "   $$\n",
    "     \\hat{\\mu}_{\\text{MC}} = \\frac{1}{N} \\sum_{i=1}^{N} f(x_i), \\quad x_i \\sim p.\n",
    "   $$\n",
    "\n",
    "2. **Estimador de Muestreo de Importancia**  \n",
    "   En lugar de muestrear de $p$, se elige una distribución propuesta $q$ que sea más sencilla de muestrear y no sea nula en regiones donde $p$ lo sea. Entonces,  \n",
    "   $$\n",
    "     \\mathbb{E}_{p}[f(X)] \n",
    "     = \\int f(x)\\,p(x)\\,dx \n",
    "     = \\int f(x)\\,\\frac{p(x)}{q(x)}\\,q(x)\\,dx \n",
    "     = \\mathbb{E}_{q}\\![f(X)\\,\\omega(X)],\n",
    "   $$\n",
    "   donde $\\omega(x)=\\frac{p(x)}{q(x)}$ es el peso de importancia. Para muestras $x_i\\sim q$, el estimador se convierte en  \n",
    "   $$\n",
    "     \\hat{\\mu}_{\\text{IS}}\n",
    "     = \\frac{1}{N}\\sum_{i=1}^{N} f(x_i)\\,\\omega(x_i).\n",
    "   $$\n",
    "\n",
    "3. **Varianza y Elección de la Propuesta**  \n",
    "   El muestreo de importancia puede reducir la varianza si $q$ concentra mayor densidad donde $f(x)\\,p(x)$ es grande. Sin embargo, si $q$ no se elige adecuadamente, la varianza puede aumentar.\n",
    "\n",
    "## Ejemplo en Python (1D)\n",
    "\n",
    "En este ejemplo se considera una distribución objetivo $p(x)$ proporcional a $\\exp(-x^2)$ y se elige como propuesta $q(x)$ una distribución normal $\\mathcal{N}(0,1)$. Se estima $\\mathbb{E}_{p}[f(X)]$ para $f(x)=x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd5f52d-122a-4872-a139-4fccd672a497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación con muestreo de importancia: 0.8920752623169319\n",
      "Estimación Monte Carlo directa: 0.4960962445819775\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def target_pdf(x):\n",
    "    # p(x) ~ exp(-x^2) sin normalizar\n",
    "    return np.exp(-x**2)\n",
    "\n",
    "def proposal_pdf(x, mean=0, std=1):\n",
    "    # q(x) = N(mean, std^2)\n",
    "    return (1.0 / np.sqrt(2.0*np.pi*std**2)) * \\\n",
    "           np.exp(-0.5 * ((x - mean)/std)**2)\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "N = 10000\n",
    "mean_proposal = 0.0\n",
    "std_proposal = 1.0\n",
    "\n",
    "# Muestras de la propuesta q(x)\n",
    "samples = np.random.normal(mean_proposal, std_proposal, size=N)\n",
    "\n",
    "# Cálculo de los pesos\n",
    "weights = target_pdf(samples) / proposal_pdf(samples, mean_proposal, std_proposal)\n",
    "\n",
    "# Estimador de importancia\n",
    "estimate_is = np.mean(f(samples) * weights)\n",
    "print(\"Estimación con muestreo de importancia:\", estimate_is)\n",
    "\n",
    "# (Opcional) Estimación Monte Carlo \"directa\" (si fuera posible muestrear de p)\n",
    "# En realidad, p(x) ~ N(0, 1/2), por lo que se puede simular escalando std.\n",
    "samples_direct = np.random.normal(0, 1/np.sqrt(2), size=N)\n",
    "estimate_mc = np.mean(samples_direct**2)\n",
    "print(\"Estimación Monte Carlo directa:\", estimate_mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1d447-c64a-49bf-977a-21a3b648e909",
   "metadata": {},
   "source": [
    "## Ejemplo en Python (2D)\n",
    "\n",
    "Aquí se estima $\\mathbb{E}[f(X,Y)]$ con $(X,Y)\\sim p$ tal que $p(x,y)\\propto \\exp(-(x^2 + y^2))$. Se elige una propuesta gaussiana 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c120d1f-4926-4962-a36d-c9337d30f87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación 2D con muestreo de importancia: 3.120392584335414\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def target_pdf_2d(x):\n",
    "    # x es [x, y]\n",
    "    return np.exp(-(x[0]**2 + x[1]**2))\n",
    "\n",
    "def proposal_pdf_2d(x, mean, cov):\n",
    "    diff = np.array(x) - np.array(mean)\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    norm_const = 1.0 / (2.0 * np.pi * np.sqrt(np.linalg.det(cov)))\n",
    "    return norm_const * np.exp(-0.5 * diff.T @ inv_cov @ diff)\n",
    "\n",
    "def f_2d(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "N = 10000\n",
    "mean_prop = [0, 0]\n",
    "cov_prop = [[1, 0],\n",
    "            [0, 1]]\n",
    "\n",
    "# Muestras de la propuesta\n",
    "samples_2d = np.random.multivariate_normal(mean_prop, cov_prop, size=N)\n",
    "\n",
    "# Pesos\n",
    "weights_2d = np.array([\n",
    "    target_pdf_2d(s) / proposal_pdf_2d(s, mean_prop, cov_prop) \n",
    "    for s in samples_2d\n",
    "])\n",
    "\n",
    "# Estimación\n",
    "estimate_is_2d = np.mean([f_2d(s)*w for s,w in zip(samples_2d, weights_2d)])\n",
    "print(\"Estimación 2D con muestreo de importancia:\", estimate_is_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f903c2-1ed8-4396-899d-881bbadc332e",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "1. **Integral 1D con Distintas Propuestas**  \n",
    "   - Definir $p(x)\\propto \\exp(-x^2)$.  \n",
    "   - Escoger dos propuestas distintas, por ejemplo, $\\mathcal{N}(0,1)$ y $\\mathcal{N}(2,1)$.  \n",
    "   - Implementar muestreo de importancia con ambas y comparar el valor estimado y su varianza.  \n",
    "   - Discutir cuál propuesta resulta más eficaz y por qué.\n",
    "\n",
    "2. **Ejemplo 2D y Visualización de Pesos**  \n",
    "   - Sea $p(x,y)\\propto \\exp(-(x^2 + y^2))$.  \n",
    "   - Usar como propuesta una distribución 2D desviada respecto al origen.  \n",
    "   - Calcular y graficar los pesos $\\omega_i$.  \n",
    "   - Analizar la presencia de valores extremos en los pesos y su efecto en la varianza.\n",
    "\n",
    "3. **Probabilidad de Evento Raro**  \n",
    "   - Calcular la probabilidad $P(X>3)$ donde $X\\sim \\mathcal{N}(0,1)$ usando Monte Carlo estándar.  \n",
    "   - Emplear muestreo de importancia usando $\\mathcal{N}(3,1)$ como propuesta.  \n",
    "   - Comparar resultados y varianzas.  \n",
    "   - Experimentar con $\\mathcal{N}(10,1)$ y observar el efecto de una propuesta alejada.\n",
    "\n",
    "4. **Escenario con Cobertura Parcial**  \n",
    "   - Definir $p(x)\\propto \\exp(-x)$ en $(0,\\infty)$.  \n",
    "   - Escoger $q(x)$ mal diseñada que no cubra toda la región positiva.  \n",
    "   - Implementar muestreo de importancia y observar el sesgo.  \n",
    "   - Corregir la propuesta para cubrir $(0,\\infty)$ y comparar resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328bc2f-76e6-4c4e-84b0-02c2c2cbce13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a130879-73a4-4745-971c-6112c2033089",
   "metadata": {},
   "source": [
    "# 3. Muestreo Estratificado\n",
    "\n",
    "Los métodos de Monte Carlo estiman integrales o valores esperados muestreando aleatoriamente una función sobre un dominio. Aunque el muestreo aleatorio simple converge con una tasa proporcional a $1 / \\sqrt{n}$, puede presentar bastante varianza en etapas tempranas o para funciones con regiones de alta variabilidad. El **muestreo estratificado** busca reducir dicha varianza dividiendo el dominio de integración en subregiones (estratos) y muestreando cada subregión de forma más controlada.\n",
    "\n",
    "Puntos clave:\n",
    "\n",
    "- En un muestreo Monte Carlo simple, las muestras pueden reunirse de manera desigual, ignorando la variabilidad en ciertas partes del dominio.  \n",
    "- El muestreo estratificado divide el dominio en subregiones (estratos), asegurando muestras en cada estrato para cubrir el dominio de forma más sistemática.  \n",
    "- A menudo, la varianza disminuye en comparación con el muestreo completamente aleatorio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f60aa4-74e8-4eda-8f0e-5f49c4b45832",
   "metadata": {},
   "source": [
    "## Formulación Matemática\n",
    "\n",
    "Sea el objetivo estimar\n",
    "$$\n",
    "I = \\int_{\\Omega} f(\\mathbf{x}) \\, d\\mathbf{x},\n",
    "$$\n",
    "donde $\\Omega$ es el dominio de integración. El muestreo estratificado particiona $\\Omega$ en $k$ estratos disjuntos $\\Omega_1, \\Omega_2, \\dots, \\Omega_k$ tales que\n",
    "$$\n",
    "\\bigcup_{j=1}^k \\Omega_j = \\Omega, \\quad \n",
    "\\Omega_j \\cap \\Omega_{j'} = \\varnothing \\quad (j \\neq j').\n",
    "$$\n",
    "Sea $p_j = \\text{Vol}(\\Omega_j)/\\text{Vol}(\\Omega)$ la fracción de “volumen” total que aporta el estrato $\\Omega_j$. Si se toman $n_j$ muestras en cada estrato, una elección común es la asignación proporcional: $n_j = n \\cdot p_j$.\n",
    "\n",
    "Dentro de cada estrato $\\Omega_j$, se muestrean puntos $\\mathbf{x}_{j,i}$ ($i = 1, \\ldots, n_j$) y se estima\n",
    "$$\n",
    "I_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} \\frac{f(\\mathbf{x}_{j,i})}{q_j(\\mathbf{x}_{j,i})},\n",
    "$$\n",
    "donde $q_j(\\mathbf{x})$ es la función de densidad de muestreo restringida al estrato $\\Omega_j$. El estimador estratificado global es\n",
    "$$\n",
    "\\hat{I} = \\sum_{j=1}^k p_j \\, I_j.\n",
    "$$\n",
    "Bajo condiciones suaves, este estimador tiene una varianza menor que el muestreo Monte Carlo simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b831a22-20a9-4d18-96e8-cddc2b6d5c13",
   "metadata": {},
   "source": [
    "### Ejemplo Unidimensional\n",
    "\n",
    "**Objetivo**: Estimar\n",
    "$$\n",
    "I = \\int_0^1 g(x)\\, dx\n",
    "$$\n",
    "usando muestreo estratificado.\n",
    "\n",
    "1. **Definir la función integrando** (por ejemplo, $g(x) = \\sin(5x)$ u otra).  \n",
    "2. **Particionar** $[0,1]$ en $k$ estratos de igual tamaño ($\\frac{1}{k}$).  \n",
    "3. **Tomar** $\\frac{n}{k}$ muestras en cada subintervalo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f21d4b-3fad-4754-95e9-58c705a3910e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación con muestreo estratificado (1D): 0.14389503324986236\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def g_1d(x):\n",
    "    return np.sin(5 * x)  # Función de ejemplo\n",
    "\n",
    "def muestreo_estratificado_1d(num_muestras=1000, k=10):\n",
    "    # k estratos, cada uno de tamaño 1/k\n",
    "    muestras_por_estrato = num_muestras // k\n",
    "    estimaciones = []\n",
    "\n",
    "    for j in range(k):\n",
    "        x_inferior = j / k\n",
    "        x_superior = (j + 1) / k\n",
    "\n",
    "        # Tomar muestras en el subintervalo\n",
    "        xs = np.random.uniform(x_inferior, x_superior, muestras_por_estrato)\n",
    "        valores = g_1d(xs)\n",
    "\n",
    "        # Promedio local en este estrato\n",
    "        promedio_local = np.mean(valores)\n",
    "\n",
    "        # Ponderado por el tamaño del estrato\n",
    "        estimaciones.append(promedio_local / k)\n",
    "\n",
    "    return np.sum(estimaciones)\n",
    "\n",
    "# Ejecución\n",
    "n = 10000\n",
    "estimacion_1d = muestreo_estratificado_1d(num_muestras=n, k=10)\n",
    "print(\"Estimación con muestreo estratificado (1D):\", estimacion_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98c9ef-cab5-4049-ae11-4dbb555b8e58",
   "metadata": {},
   "source": [
    "### Ejemplo Bidimensional\n",
    "\n",
    "**Objetivo**: Estimar\n",
    "$$\n",
    "I = \\iint_{[0,1]^2} f(x,y)\\, dx\\, dy\n",
    "$$\n",
    "con muestreo estratificado. Por ejemplo,\n",
    "$$\n",
    "f(x, y) = \\exp\\bigl(-(x^2 + y^2)\\bigr).\n",
    "$$\n",
    "\n",
    "1. **Particionar** el cuadrado unitario $[0,1]^2$ en una malla de $k \\times k$ subcuadrados.  \n",
    "2. **Muestrear** en cada subcuadrado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21fa8956-fb9d-44dc-8d94-31db29a2f125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación con muestreo estratificado (2D): 0.5578892505005325\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f_2d(x, y):\n",
    "    return np.exp(-(x**2 + y**2))\n",
    "\n",
    "def muestreo_estratificado_2d(num_muestras=10000, k=10):\n",
    "    # k x k subcuadrados\n",
    "    muestras_por_celda = num_muestras // (k * k)\n",
    "    estimacion_total = 0.0\n",
    "\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            x_inferior, x_superior = i / k, (i + 1) / k\n",
    "            y_inferior, y_superior = j / k, (j + 1) / k\n",
    "\n",
    "            # Muestra en la subcelda\n",
    "            xs = np.random.uniform(x_inferior, x_superior, muestras_por_celda)\n",
    "            ys = np.random.uniform(y_inferior, y_superior, muestras_por_celda)\n",
    "\n",
    "            valores = f_2d(xs, ys)\n",
    "            promedio_local = np.mean(valores)\n",
    "\n",
    "            # Cada subcuadrado tiene área 1/(k^2)\n",
    "            area_subcuadrado = 1.0 / (k**2)\n",
    "            estimacion_total += promedio_local * area_subcuadrado\n",
    "\n",
    "    return estimacion_total\n",
    "\n",
    "estimacion_2d = muestreo_estratificado_2d(num_muestras=10000, k=10)\n",
    "print(\"Estimación con muestreo estratificado (2D):\", estimacion_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176b4e6-6fc6-4486-b66f-e0452ca6d135",
   "metadata": {},
   "source": [
    "## Ejercicios Propuestos\n",
    "\n",
    "1. **Muestreo Estratificado vs. Muestreo Crudo (1D)**  \n",
    "   - Implementa tanto el muestreo crudo como el estratificado para estimar la integral de una función a tu elección en [0,1].  \n",
    "   - Repite cada enfoque varias veces (por ejemplo, 50 a 100 repeticiones) y registra la desviación estándar de las estimaciones.  \n",
    "   - Compara las medias y desviaciones estándar de ambos métodos y explica por qué el muestreo estratificado puede reducir la varianza.\n",
    "\n",
    "2. **Optimización del Número de Estratos**  \n",
    "   - En 1D, varía el número de estratos k (por ejemplo, 2, 5, 10, 20) para un número fijo de muestras totales.  \n",
    "   - Para cada k, estima la integral (puedes usar más de una función) y mide el error promedio y la varianza a lo largo de varias repeticiones.  \n",
    "   - Haz un gráfico o tabla de la varianza en función de k, y comenta cómo el incremento de k influye en los resultados.  \n",
    "   - Identifica un punto en el que incrementar k deja de brindar beneficios significativos y argumenta posibles razones.\n",
    "\n",
    "3. **Integración Estratificada en 2D**  \n",
    "   - Escribe código para integrar una función 2D en [0,1]×[0,1] usando muestreo estratificado.  \n",
    "   - Compara el resultado con (1) muestreo crudo y (2) una solución analítica (si la función lo permite).  \n",
    "   - Sub-ejercicio: Intenta utilizar más estratos en el eje donde la función muestre mayor variabilidad. Describe el efecto sobre la reducción de la varianza.\n",
    "\n",
    "4. **Practicidad en Dimensiones Elevadas**  \n",
    "   - Extiende tu código a 3D o 4D, usando el mismo número total de muestras.  \n",
    "   - Observa cuán rápido crece el número de estratos y cómo esto impacta los tiempos de ejecución.  \n",
    "   - Sub-ejercicio: Introduce un enfoque adaptativo o jerárquico (por ejemplo, subdividir más en estratos de alta varianza). Compara la precisión y el tiempo de ejecución con el estratificado uniforme, y describe las compensaciones observadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f6cb5f-727a-4554-ad25-459ab95bc9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c83a911f-8cdc-451f-a09f-8245b0056bf2",
   "metadata": {},
   "source": [
    "# Variantes de Control\n",
    "\n",
    "Las **variantes de control** (control variates) son una técnica potente para reducir la varianza en simulaciones de Monte Carlo. La idea principal consiste en introducir una variable aleatoria auxiliar (el “control”) cuya esperanza es conocida y que está correlacionada con la cantidad de interés. Al restar una forma ponderada de ese control de nuestro estimador original, se puede obtener una estimación de menor varianza sin cambiar el valor esperado.\n",
    "\n",
    "**Puntos clave a introducir:**\n",
    "\n",
    "- Cómo los estimadores de Monte Carlo (MC) pueden presentar varianzas altas, especialmente en sistemas complejos.  \n",
    "- El concepto central de variantes de control: sumar y restar una función de media conocida que además esté correlacionada con la función objetivo.  \n",
    "- Cómo la correlación entre la variante de control y la función objetivo determina la reducción de varianza.  \n",
    "- La fórmula para el peso óptimo.\n",
    "\n",
    "## Formulación Matemática\n",
    "\n",
    "Sea $$X$$ una variable aleatoria con distribución $$p(x)$$. Queremos estimar\n",
    "$$\n",
    "\\theta \\;=\\; \\mathbb{E}[V(X)] \\;=\\; \\int V(x)\\,p(x)\\,dx.\n",
    "$$\n",
    "El estimador de Monte Carlo estándar es\n",
    "$$\n",
    "\\hat{\\theta} \\;=\\; \\frac{1}{N}\\sum_{k=1}^N V(X_k),\n",
    "$$\n",
    "donde $X_1, X_2, \\dots, X_N$ son muestras independientes de $p(x)$.\n",
    "\n",
    "Una variante de control es una función $W(X)$ con esperanza conocida $\\mathbb{E}[W(X)] = \\mu$. Definimos:\n",
    "$$\n",
    "U(X) \\;=\\; V(X) \\;-\\; \\alpha \\bigl(W(X) - \\mu \\bigr),\n",
    "$$\n",
    "donde $\\alpha$ es una constante que debemos elegir. El nuevo estimador de Monte Carlo se vuelve:\n",
    "$$\n",
    "\\hat{\\theta}_{\\text{cv}} \\;=\\; \\frac{1}{N}\\sum_{k=1}^N \\Bigl(V(X_k) - \\alpha\\bigl(W(X_k) - \\mu\\bigr)\\Bigr) + \\alpha\\,\\mu.\n",
    "$$\n",
    "La esperanza de este estimador sigue siendo $\\theta$ para cualquier $\\alpha$. Su varianza se minimiza cuando\n",
    "$$\n",
    "\\alpha^* \\;=\\; \\frac{\\mathrm{Cov}(V(X),\\,W(X))}{\\mathrm{Var}(W(X))}.\n",
    "$$\n",
    "En la práctica, $\\alpha^*$ se desconoce y debe estimarse usando las mismas muestras de Monte Carlo o corridas de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfecab1-5621-441b-8ddf-6e58dc80848f",
   "metadata": {},
   "source": [
    "## Ejemplos en Python paso a paso\n",
    "\n",
    "A continuación se muestran fragmentos de código ilustrativos. Se enfoca en la generación de muestras, la aplicación de la variante de control y la estimación de $\\alpha$. El nivel de detalle puede ajustarse según la audiencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c4aa84-f62d-4f30-9527-65fc1ad5bebc",
   "metadata": {},
   "source": [
    "## Ejemplo en 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "712d02bb-4270-49b8-960d-5d02c244e768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación MC estándar: 0.789904607451731\n",
      "Estimación con variante de control: 1.3890139361473501\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def funcion_objetivo(x):\n",
    "    # Supongamos que queremos la esperanza de x^2 + np.sin(x)\n",
    "    return x**2 + np.sin(x)\n",
    "\n",
    "def funcion_control(x):\n",
    "    # Usamos x^2 como control, cuya media en [0,1] se conoce\n",
    "    return x**2\n",
    "\n",
    "N = 10_000\n",
    "X = np.random.rand(N)  # Muestras uniformes en [0,1]\n",
    "\n",
    "V_muestras = funcion_objetivo(X)\n",
    "W_muestras = funcion_control(X)\n",
    "\n",
    "# Media verdadera de x^2 en [0,1] = 1/3\n",
    "media_control = 1/3\n",
    "\n",
    "cov_VW = np.cov(V_muestras, W_muestras, bias=True)[0,1]\n",
    "var_W = np.var(W_muestras, ddof=0)\n",
    "\n",
    "alpha = cov_VW / var_W\n",
    "\n",
    "estimacion_cv = np.mean(V_muestras - alpha*(W_muestras - media_control)) + alpha*media_control\n",
    "estimacion_mc = np.mean(V_muestras)\n",
    "\n",
    "print(\"Estimación MC estándar:\", estimacion_mc)\n",
    "print(\"Estimación con variante de control:\", estimacion_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ff850-76a4-49ba-b68c-5ed7ab31ad40",
   "metadata": {},
   "source": [
    "## Ejemplo en 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a52381d9-fdeb-4ee1-bdb2-88a1acc20dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación MC estándar (2D): -0.006121597779753133\n",
      "Estimación con variante de control (2D): -0.008071857483568009\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def funcion_objetivo_2d(x, y):\n",
    "    # Ejemplo: x*y + np.sin(x+y)\n",
    "    return x*y + np.sin(x+y)\n",
    "\n",
    "def funcion_control_2d(x, y):\n",
    "    # Usamos x*y como control. Si X e Y son N(0,1) independientes, la media es 0\n",
    "    return x*y\n",
    "\n",
    "N = 10_000\n",
    "X = np.random.randn(N)\n",
    "Y = np.random.randn(N)\n",
    "\n",
    "V_muestras_2d = funcion_objetivo_2d(X, Y)\n",
    "W_muestras_2d = funcion_control_2d(X, Y)\n",
    "\n",
    "media_control_2d = 0.0\n",
    "\n",
    "cov_VW_2d = np.cov(V_muestras_2d, W_muestras_2d, bias=True)[0,1]\n",
    "var_W_2d = np.var(W_muestras_2d, ddof=0)\n",
    "\n",
    "alpha_2d = cov_VW_2d / var_W_2d if var_W_2d != 0 else 0\n",
    "\n",
    "estimacion_cv_2d = np.mean(V_muestras_2d - alpha_2d*(W_muestras_2d - media_control_2d)) + alpha_2d*media_control_2d\n",
    "estimacion_mc_2d = np.mean(V_muestras_2d)\n",
    "\n",
    "print(\"Estimación MC estándar (2D):\", estimacion_mc_2d)\n",
    "print(\"Estimación con variante de control (2D):\", estimacion_cv_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcba99f-4766-4c56-b3a1-ee16a6962c32",
   "metadata": {},
   "source": [
    "## Ejemplo en ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea1bacd7-ed7d-4763-a6cf-8a9d7c22d6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación MC estándar (d=5): 0.00015350407578632836\n",
      "Estimación con variante de control (d=5): -0.0028752864726057665\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def funcion_objetivo_nd(muestras):\n",
    "    # 'muestras' tendrá forma (N, d)\n",
    "    # Ejemplo: suma de sin en cada dimensión\n",
    "    return np.sum(np.sin(muestras), axis=1)\n",
    "\n",
    "def funcion_control_nd(muestras):\n",
    "    # Usamos la suma de los valores como control\n",
    "    return np.sum(muestras, axis=1)\n",
    "\n",
    "def variantes_control_nd(d, N=10_000):\n",
    "    # Creamos muestras de dimensión d de una normal estándar\n",
    "    X = np.random.randn(N, d)\n",
    "\n",
    "    V_muestras = funcion_objetivo_nd(X)\n",
    "    W_muestras = funcion_control_nd(X)\n",
    "\n",
    "    # La media de la suma de d normales estándar es 0\n",
    "    media_control = 0.0\n",
    "\n",
    "    cov_VW = np.cov(V_muestras, W_muestras, bias=True)[0,1]\n",
    "    var_W = np.var(W_muestras, ddof=0)\n",
    "    alpha = cov_VW / var_W if var_W != 0 else 0\n",
    "\n",
    "    estimacion_cv = np.mean(V_muestras - alpha*(W_muestras - media_control)) + alpha*media_control\n",
    "    estimacion_mc = np.mean(V_muestras)\n",
    "    \n",
    "    return estimacion_mc, estimacion_cv\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    d = 5\n",
    "    mc_est, cv_est = variantes_control_nd(d, N=20_000)\n",
    "    print(f\"Estimación MC estándar (d={d}):\", mc_est)\n",
    "    print(f\"Estimación con variante de control (d={d}):\", cv_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752be1e7-e159-4142-983e-4624068d8f6a",
   "metadata": {},
   "source": [
    "## Ejercicios Propuestos\n",
    "\n",
    "**Ejercicio 1**  \n",
    "\n",
    "1. Implementa un estimador de Monte Carlo para aproximar la integral  \n",
    "   $$\\int_{0}^{1} x^2 \\, dx.$$  \n",
    "   Compara con la solución exacta.  \n",
    "2. Preguntas:  \n",
    "   - ¿Cómo cambia la varianza de la estimación con diferentes tamaños de muestra?  \n",
    "   - ¿Qué factores influyen en la precisión del resultado?\n",
    "\n",
    "**Ejercicio 2**  \n",
    "\n",
    "1. Aplica una variante de control al mismo estimador del Ejercicio 1.  \n",
    "   - Usa $W(x) = x$ como función de control (media conocida: 0.5 en [0,1]).  \n",
    "2. Preguntas:  \n",
    "   - Estima $\\alpha$ a partir de las muestras. ¿Cuánto mejora el resultado en comparación con el método estándar?  \n",
    "   - ¿Qué pasa con la reducción de varianza si cambias $W(x)$ por una función poco correlacionada?\n",
    "\n",
    "**Ejercicio 3**  \n",
    "\n",
    "1. Pasa a un escenario 2D donde $(X, Y)$ siguen una distribución normal bivariante con correlación $\\rho$.  \n",
    "   - Define una función $V(X, Y)$.  \n",
    "   - Elige una función de control $W(X, Y)$ con media conocida.  \n",
    "2. Preguntas:  \n",
    "   - ¿Cómo se calcula $\\mathrm{Cov}(V, W)$ en la práctica?  \n",
    "   - ¿Cómo afecta el valor de $\\rho$ a la efectividad de la variante de control?  \n",
    "   - ¿Cómo se compara la reducción de varianza para diferentes valores de $\\rho$?\n",
    "\n",
    "**Ejercicio 4**  \n",
    "\n",
    "1. Aplica variantes de control en un caso de mayor dimensión (3D o más).  \n",
    "   - Por ejemplo, genera muestras de $\\mathcal{N}(0, I)$ en dimensión $d$.  \n",
    "   - Define $V(\\mathbf{X}) = \\sum \\sin(X_i)$ o una función similar.  \n",
    "2. Preguntas:  \n",
    "   - Propón una variante de control y explica por qué podría estar correlacionada con $V$.  \n",
    "   - Compara el desempeño de tu método contra el Monte Carlo estándar para distintas dimensiones.  \n",
    "   - ¿Cuáles son las dificultades al estimar $\\alpha$ en alta dimensión y cómo podrías mitigarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08317037-2e1d-422f-a382-e8833b6289be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0ce0954-ff46-4fc5-b3cf-9b2bd8070636",
   "metadata": {},
   "source": [
    "# Variantes Antitéticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a724f94e-0401-4493-94c1-fbb864529140",
   "metadata": {},
   "source": [
    "Los métodos de Monte Carlo (MC) aproximan integrales o expectativas muestreando al azar desde una distribución de probabilidad y tomando promedios empíricos. La técnica de *variantes antitéticas* (Antithetic Variates) es un método de **reducción de varianza**. En lugar de muestrear puntos completamente al azar, se generan pares de entradas aleatorias correlacionadas negativamente. Esta correlación reduce la fluctuación estadística del estimador, lo que puede dar resultados más precisos con el mismo número de muestras.\n",
    "\n",
    "Cuando se usan variantes antitéticas:\n",
    "\n",
    "1. Cada punto aleatorio $U$ se empareja con un punto transformado $\\bar{U}$ de modo que ambos estén correlacionados negativamente.  \n",
    "2. Se calcula el promedio de la función en ambos puntos, reduciendo así la varianza global del estimador.\n",
    "\n",
    "A continuación, se muestra una propuesta de plan de lección que incluye ejemplos en Python para sistemas de 1D, 2D y ND, así como análisis de las dificultades potenciales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84ae3d2-2ccf-4ce9-96a3-d8b005f5b482",
   "metadata": {},
   "source": [
    "## Formulación Matemática\n",
    "\n",
    "Dada una muestra aleatoria $\\mathbf{Z}$, se construye su pareja antitética $\\mathbf{Z}^*$ (a menudo $\\mathbf{Z}^* = -\\mathbf{Z}$ en distribuciones simétricas). Si se desea estimar $\\mu = \\mathbb{E}[f(\\mathbf{Z})]$, el estimador antitético es:\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{f(\\mathbf{Z}_i) + f(\\mathbf{Z}_i^*)}{2}.\n",
    "$$\n",
    "Si $\\operatorname{Cov}(f(\\mathbf{Z}), f(\\mathbf{Z}^*)) < 0$, la varianza del promedio $\\tfrac{1}{2}\\bigl(f(\\mathbf{Z}) + f(\\mathbf{Z}^*)\\bigr)$ es menor que la varianza de usar un solo punto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7079a-1438-42b6-8c1a-9b6645601c14",
   "metadata": {},
   "source": [
    "### Ejemplo Unidimensional en Python**\n",
    "\n",
    "Supongamos que queremos estimar la integral:\n",
    "$$\n",
    "I = \\int_0^1 x^3 \\, dx.\n",
    "$$\n",
    "El valor exacto es $I = 0.25$. Implementamos un estimador Monte Carlo con y sin variantes antitéticas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09342225-131a-469c-8778-b193d5d430a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor exacto:               0.25\n",
      "Estimación MC estándar:     0.2474158062722032\n",
      "Estimación antitética:      0.2492623694418049\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return x**3\n",
    "\n",
    "N = 10_000\n",
    "\n",
    "# Monte Carlo estándar\n",
    "u = np.random.rand(N)\n",
    "estimate_mc = np.mean(f(u))\n",
    "\n",
    "# Variantes antitéticas\n",
    "# Emparejamos u_i con 1 - u_i\n",
    "u1 = np.random.rand(N//2)\n",
    "u2 = 1.0 - u1\n",
    "estimate_av = np.mean( (f(u1) + f(u2))/2.0 )\n",
    "\n",
    "print(\"Valor exacto:              \", 0.25)\n",
    "print(\"Estimación MC estándar:    \", estimate_mc)\n",
    "print(\"Estimación antitética:     \", estimate_av)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4aadf-1cd8-4953-b3e7-3958e965de85",
   "metadata": {},
   "source": [
    "1. Observar si la estimación antitética fluctúa menos que la estimación Monte Carlo estándar.  \n",
    "2. Explicar cómo surge la correlación negativa, ya que $x + (1-x)=1$ produce correlación negativa para muchas funciones monótonas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286d90a-814e-4873-9dcb-0840bea21a72",
   "metadata": {},
   "source": [
    "### Ejemplo Bidimensional**\n",
    "\n",
    "Consideremos:\n",
    "$$\n",
    "I = \\iint_{[0,1]^2} (x + y)^3 \\, dx\\,dy.\n",
    "$$\n",
    "Podemos calcular el valor exacto analíticamente, pero usaremos Monte Carlo para ilustrar. Sea $(U_1, U_2)$ uniforme en $[0,1]^2$. Su pareja antitética es $(1-U_1, 1-U_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "037791d1-b885-4096-b1e9-3d21dd8aeaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aprox (MC estándar):   1.4672723383513167\n",
      "Aprox (antitético):    1.5057870918793264\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f_2d(x, y):\n",
    "    return (x + y)**3\n",
    "\n",
    "N = 10_000\n",
    "u = np.random.rand(N, 2)  # shape (N,2)\n",
    "\n",
    "# MC estándar\n",
    "vals_mc = [f_2d(*pt) for pt in u]\n",
    "estimate_mc_2d = np.mean(vals_mc)\n",
    "\n",
    "# Antitético\n",
    "u_half = np.random.rand(N//2, 2)\n",
    "u_antithetic = 1.0 - u_half\n",
    "vals_av = [(f_2d(*pt1) + f_2d(*pt2))/2.0 \n",
    "           for pt1, pt2 in zip(u_half, u_antithetic)]\n",
    "estimate_av_2d = np.mean(vals_av)\n",
    "\n",
    "print(\"Aprox (MC estándar):  \", estimate_mc_2d)\n",
    "print(\"Aprox (antitético):   \", estimate_av_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042eca5a-b6ab-4a24-90e3-30feb4b1ce7c",
   "metadata": {},
   "source": [
    "### Ejemplo ND\n",
    "\n",
    "En dimensiones más altas (por ej., $d$ dimensiones uniforme en $[0,1]^d$), la construcción es similar. Si el vector aleatorio es $\\mathbf{U} = (U_1, \\dots, U_d)$, la versión antitética es $(1-U_1, \\dots, 1-U_d)$.\n",
    "\n",
    "Aquí hay un ejemplo en Python para la función $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2$ en dimensión $d$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d829e174-471f-4f3a-953c-a342e73edd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión: 5\n",
      "Estimación MC estándar: 1.6673688503622477\n",
      "Estimación antitética:  1.6681010924764887\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f_nd(x):\n",
    "    # x es un array 1D con dimensión d\n",
    "    return np.sum(x**2)\n",
    "\n",
    "d = 5     # dimensión\n",
    "N = 20_000\n",
    "\n",
    "U = np.random.rand(N, d)\n",
    "vals_mc_nd = [f_nd(u_i) for u_i in U]\n",
    "estimate_mc_nd = np.mean(vals_mc_nd)\n",
    "\n",
    "U_half = np.random.rand(N//2, d)\n",
    "U_antithetic = 1.0 - U_half\n",
    "vals_av_nd = [(f_nd(uh) + f_nd(ua))/2.0 \n",
    "              for uh, ua in zip(U_half, U_antithetic)]\n",
    "estimate_av_nd = np.mean(vals_av_nd)\n",
    "\n",
    "print(\"Dimensión:\", d)\n",
    "print(\"Estimación MC estándar:\", estimate_mc_nd)\n",
    "print(\"Estimación antitética: \", estimate_av_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b21f1-3c67-4166-a447-3ebc1c5cdc79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "866ce975-78b2-4491-b935-b4df1ad2b977",
   "metadata": {},
   "source": [
    "## **Ejercicios Propuestos**\n",
    "\n",
    "### **Ejercicio 1: Integral 1D con Comparación Pareada**\n",
    "\n",
    "1. Implementar un estimador Monte Carlo para aproximar  \n",
    "   $$\n",
    "   I = \\int_{0}^{1} \\frac{1}{1 + x} \\, dx \n",
    "   $$\n",
    "   usando tanto el método estándar como el antitético, donde $u' = 1 - u$.  \n",
    "\n",
    "2. Comparar sus estimaciones y varianzas empíricas en varias corridas.  \n",
    "\n",
    "3. Analizar si al aumentar el número de muestras se reduce de forma consistente la brecha de varianza.  \n",
    "\n",
    "4. Reflexionar sobre posibles situaciones donde las variantes antitéticas no reduzcan la varianza.\n",
    "\n",
    "### **Ejercicio 2: Verificación de Covarianza Negativa**\n",
    "\n",
    "1. Escoger una función $f(x)$ estrictamente creciente (por ejemplo, $f(x) = x^3$) y generar pares $(u, 1-u)$.  \n",
    "2. Calcular la covarianza muestral entre $\\{f(u_i)\\}$ y $\\{f(1-u_i)\\}$.  \n",
    "3. Verificar si la covarianza resulta negativa.  \n",
    "4. Explicar brevemente la relevancia de la covarianza en el rendimiento de las variantes antitéticas.\n",
    "\n",
    "### **Ejercicio 3: Estimación en 2D**\n",
    "\n",
    "1. Definir la integral 2D:  \n",
    "   $$\n",
    "   I = \\iint_{[0,1]^2} (x + y)^2 \\, dx \\, dy.\n",
    "   $$\n",
    "\n",
    "2. Implementar un estimador Monte Carlo estándar.  \n",
    "\n",
    "3. Implementar la versión antitética emparejando $(u_1, u_2)$ con $(1-u_1, 1-u_2)$.  \n",
    "\n",
    "4. Comparar las varianzas numéricas para ver si las variantes antitéticas ayudan.  \n",
    "\n",
    "5. Explicar por qué la magnitud de la reducción de varianza varía con diferentes funciones incluso en la misma dimensión.\n",
    "\n",
    "### **Ejercicio 4: Experimento en Altas Dimensiones y Dificultades**\n",
    "\n",
    "1. Elegir una dimensión $d \\ge 5$ y la función  \n",
    "   $$\n",
    "   f(\\mathbf{x}) = e^{-\\|\\mathbf{x}\\|^2}, \\quad \\mathbf{x} \\in [0,1]^d.\n",
    "   $$\n",
    "\n",
    "2. Crear un estimador Monte Carlo con $N$ puntos $\\mathbf{x}_i$.  \n",
    "\n",
    "3. Construir la muestra antitética $\\mathbf{x}_i^* = \\mathbf{1} - \\mathbf{x}_i$ y calcular el estimador antitético.  \n",
    "\n",
    "4. Examinar si la correlación negativa entre $f(\\mathbf{x}_i)$ y $f(\\mathbf{x}_i^*)$ reduce la varianza de forma apreciable.  \n",
    "\n",
    "5. Discutir cómo la dimensionalidad alta puede disminuir los beneficios del muestreo antitético.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4db68f-f0e7-4460-8b35-b40811e7be90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
